================================================================================
                   __   __ ___________                 _                        
                   \ \ / //  ___| ___ \               | |                       
                    \ V / \ `--.| |_/ / ___ _ __   ___| |__                     
                    /   \  `--. \ ___ \/ _ \ '_ \ / __| '_ \                    
                   / /^\ \/\__/ / |_/ /  __/ | | | (__| | | |                   
                   \/   \/\____/\____/ \___|_| |_|\___|_| |_|                   

================================================================================
                    Developed at Argonne National Laboratory
                                   Version: 20
================================================================================
                                  INPUT SUMMARY
================================================================================
Simulation Method:            History Based
Grid Type:                    Unionized Grid
Materials:                    12
H-M Benchmark Size:           large
Total Nuclides:               355
Gridpoints (per Nuclide):     11,303
Unionized Energy Gridpoints:  4,012,565
Particle Histories:           500,000
XS Lookups per Particle:      150
Total XS Lookups:             150
Threads:                      20
Est. Memory Usage (MB):       5,649
Binary File Mode:             Off
================================================================================
                         INITIALIZATION - DO NOT PROFILE
================================================================================
Intializing nuclide grids...
Intializing unionized grid...
Intializing material data...
Intialization complete. Allocated 5648 MB of data.

================================================================================
                                   SIMULATION
================================================================================
Beginning history based simulation...

Simulation complete.
================================================================================
                                     RESULTS
================================================================================
Threads:     20
Runtime:     30.455 seconds
Lookups:     75,000,000
Lookups/s:   2,462,679
Verification checksum: 792850 (WARNING - INVALID CHECKSUM!)
================================================================================
Elapsed Time: 36.853s
    CPU Time: 595.835s
    Memory Bound: 75.2% of Pipeline Slots
     | The metric value is high. This may indicate that a significant fraction
     | of execution pipeline slots could be stalled due to demand memory load
     | and stores. Explore the metric breakdown by memory hierarchy, memory
     | bandwidth information, and correlation by memory objects.
     |
        L1 Bound: 1.3% of Clockticks
        L2 Bound: 0.5% of Clockticks
        L3 Bound: 6.3% of Clockticks
         | This metric shows how often CPU was stalled on L3 cache, or contended
         | with a sibling Core. Avoiding cache misses (L2 misses/L3 hits)
         | improves the latency and increases performance.
         |
        DRAM Bound: 71.9% of Clockticks
         | This metric shows how often CPU was stalled on the main memory
         | (DRAM). Caching typically improves the latency and increases
         | performance.
         |
            DRAM Bandwidth Bound: 70.4% of Elapsed Time
             | The system spent much time heavily utilizing DRAM bandwidth.
             | Improve data accesses to reduce cacheline transfers from/to
             | memory using these possible techniques: 1) consume all bytes of
             | each cacheline before it is evicted (for example, reorder
             | structure elements and split non-hot ones); 2) merge compute-
             | limited and bandwidth-limited loops; 3) use NUMA optimizations on
             | a multi-socket system. Note: software prefetches do not help a
             | bandwidth-limited application. Run Memory Access analysis to
             | identify data structures to be allocated in High Bandwidth Memory
             | (HBM), if available.
             |
        Store Bound: 0.2% of Clockticks
        NUMA: % of Remote Accesses: 47.9%
         | A significant amount of DRAM loads were serviced from remote DRAM.
         | Wherever possible, try to consistently use data on the same core, or
         | at least the same package, as it was allocated on.
         |
        UPI Utilization Bound: 0.0% of Elapsed Time
    Loads: 88,152,644,500
    Stores: 29,435,883,050
    LLC Miss Count: 8,315,582,050
        Local Memory Access Count: 4,390,307,300
        Remote Memory Access Count: 3,615,253,050
        Remote Cache Access Count: 250,017,500
    Average Latency (cycles): 252
    Total Thread Count: 20
    Paused Time: 0s

Bandwidth Utilization
Bandwidth Domain                  Platform Maximum  Observed Maximum  Average  % of Elapsed Time with High BW Utilization(%)
--------------------------------  ----------------  ----------------  -------  ---------------------------------------------
DRAM, GB/sec                      70                          37.800   26.633                                           0.0%
DRAM Single-Package, GB/sec       35                          28.300   21.073                                          70.4%
UPI Utilization Single-link, (%)  100                         46.400   34.452                                           0.0%
Collection and Platform Info
    Application Command Line: /home/osm/workspace/ABC/app/XSBench/openmp-threading/XSBench "-s" "large" "-l" "150" 
    User Name: root
    Operating System: 5.11.0-27-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=20.04 DISTRIB_CODENAME=focal DISTRIB_DESCRIPTION="Ubuntu 20.04.3 LTS"
    Computer Name: osm-X11DAi-N
    Result Size: 216.7 MB 
    Collection start time: 06:45:25 05/08/2024 UTC
    Collection stop time: 06:46:01 05/08/2024 UTC
    Collector Type: Event-based sampling driver
    CPU
        Name: Intel(R) Xeon(R) Processor code named Cascadelake
        Frequency: 2.400 GHz
        Logical CPU Count: 20
        Max DRAM Single-Package Bandwidth: 35.000 GB/s
        LLC size: 14.4 MB 
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
