Generate Time:       139.82878
Build Time:          213.35852
Graph has 268435453 nodes and 4236159892 undirected edges for degree: 15
Trial Time:          56.56768
Trial Time:          56.42886
Average Time:        56.49827
Elapsed Time: 506.024s
    CPU Time: 9124.575s
    Memory Bound: 65.5% of Pipeline Slots
     | The metric value is high. This may indicate that a significant fraction
     | of execution pipeline slots could be stalled due to demand memory load
     | and stores. Explore the metric breakdown by memory hierarchy, memory
     | bandwidth information, and correlation by memory objects.
     |
        L1 Bound: 33.2% of Clockticks
         | This metric shows how often machine was stalled without missing the
         | L1 data cache. The L1 cache typically has the shortest latency.
         | However, in certain cases like loads blocked on older stores, a load
         | might suffer a high latency even though it is being satisfied by the
         | L1.
         |
        L2 Bound: 0.0% of Clockticks
        L3 Bound: 5.1% of Clockticks
         | This metric shows how often CPU was stalled on L3 cache, or contended
         | with a sibling Core. Avoiding cache misses (L2 misses/L3 hits)
         | improves the latency and increases performance.
         |
        DRAM Bound: 36.7% of Clockticks
         | This metric shows how often CPU was stalled on the main memory
         | (DRAM). Caching typically improves the latency and increases
         | performance.
         |
            DRAM Bandwidth Bound: 22.2% of Elapsed Time
             | The system spent much time heavily utilizing DRAM bandwidth.
             | Improve data accesses to reduce cacheline transfers from/to
             | memory using these possible techniques: 1) consume all bytes of
             | each cacheline before it is evicted (for example, reorder
             | structure elements and split non-hot ones); 2) merge compute-
             | limited and bandwidth-limited loops; 3) use NUMA optimizations on
             | a multi-socket system. Note: software prefetches do not help a
             | bandwidth-limited application. Run Memory Access analysis to
             | identify data structures to be allocated in High Bandwidth Memory
             | (HBM), if available.
             |
        Store Bound: 0.3% of Clockticks
        NUMA: % of Remote Accesses: 53.0%
         | A significant amount of DRAM loads were serviced from remote DRAM.
         | Wherever possible, try to consistently use data on the same core, or
         | at least the same package, as it was allocated on.
         |
        UPI Utilization Bound: 0.0% of Elapsed Time
    Loads: 1,280,648,418,300
    Stores: 522,630,678,450
    LLC Miss Count: 70,389,926,950
        Local Memory Access Count: 30,747,152,150
        Remote Memory Access Count: 36,987,588,950
        Remote Cache Access Count: 2,575,180,250
    Average Latency (cycles): 82
    Total Thread Count: 20
    Paused Time: 0s

Bandwidth Utilization
Bandwidth Domain                  Platform Maximum  Observed Maximum  Average  % of Elapsed Time with High BW Utilization(%)
--------------------------------  ----------------  ----------------  -------  ---------------------------------------------
DRAM, GB/sec                      70                          63.300   21.218                                           1.8%
DRAM Single-Package, GB/sec       35                          31.800   11.736                                          22.2%
UPI Utilization Single-link, (%)  100                         50.900   21.545                                           0.0%
Collection and Platform Info
    Application Command Line: /home/osm/workspace/ABC/app/gapbs/pr "-g" "28" "-n" "2" 
    User Name: root
    Operating System: 5.11.0-27-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=20.04 DISTRIB_CODENAME=focal DISTRIB_DESCRIPTION="Ubuntu 20.04.3 LTS"
    Computer Name: osm-X11DAi-N
    Result Size: 2.0 GB 
    Collection start time: 11:27:56 06/08/2024 UTC
    Collection stop time: 11:36:22 06/08/2024 UTC
    Collector Type: Event-based sampling driver
    CPU
        Name: Intel(R) Xeon(R) Processor code named Cascadelake
        Frequency: 2.400 GHz
        Logical CPU Count: 20
        Max DRAM Single-Package Bandwidth: 35.000 GB/s
        LLC size: 14.4 MB 
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
Intel(R) VTune(TM) Profiler 2024.2.0 (build 628218) feedback tool
Copyright (C) 2009 Intel Corporation. All rights reserved.

Intel(R) VTune(TM) Profiler 2024.2.0; 628218 experienced an unexpected error.

Please send a problem report: 
amplxe-feedback --send-crash-report "/tmp/amplxe-log-root/2024-08-06-20-27-48-975938.vtune/crash_info.txt"

Report data may be used to improve product stability. Our apologies for the inconvenience and thank you for your assistance.

Exception: 0xb, Segmentation&nbsp;fault
Module: libswip.so
Collecting modules information...
Collecting system information...
